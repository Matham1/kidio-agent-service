## âœ… AI Agent Microservice

### Project Structure

```
ai_microservice/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          â† Pydantic v2 Settings (all env vars)
â”‚   â””â”€â”€ tracking.py        â† MLflow async context manager
â”œâ”€â”€ proto/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ agent.proto        â† gRPC service + messages
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag_service.py     â† BaseRetriever (abstract) + DummyRetriever
â”‚   â”œâ”€â”€ llm_service.py     â† Async Ollama client (httpx + retries)
â”‚   â””â”€â”€ orchestrator.py    â† Central coordination: RAG â†’ LLM â†’ MLflow
â”œâ”€â”€ transport/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ grpc_server.py     â† gRPC AgentServiceServicer
â”‚   â””â”€â”€ rest_api.py        â† FastAPI router (POST /generate, GET /health)
â”œâ”€â”€ main.py                â† FastAPI entrypoint (port 8002)
â”œâ”€â”€ run_grpc.py            â† gRPC entrypoint (port 50051)
â”œâ”€â”€ start.sh               â† Dual-server launcher (both ports)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile             â† python:3.11-slim, auto-compiles protobuf
â”œâ”€â”€ docker-compose.yml     â† ai-agent + ollama + mlflow
â”œâ”€â”€ .env.example
â”œâ”€â”€ .env
â”œâ”€â”€ .dockerignore
â””â”€â”€ .gitignore
```

### Architecture

```
Django â”€â”€(gRPC :50051)â”€â”€â–¶ ai-agent â”€â”€(HTTP)â”€â”€â–¶ ollama (Qwen2.5-7B)
                              â”‚
                              â””â”€â”€(HTTP)â”€â”€â–¶ mlflow (:5000)
```

### Key Design Decisions

| Concern | Implementation |
|---|---|
| **Dual transport** | `start.sh` runs both FastAPI + gRPC concurrently in one container |
| **RAG abstraction** | `BaseRetriever` ABC â†’ inject `PgVectorRetriever` / `QdrantRetriever` later |
| **LLM resilience** | `tenacity` retries (3x exponential backoff) + configurable timeout |
| **MLflow tracking** | Every generation logged (params, prompts, output, latency) via async context manager |
| **Structured output** | `generate_structured()` parses JSON, gracefully degrades |
| **GPU readiness** | Commented `deploy.resources.reservations` block in docker-compose |
| **Container-safe** | All URLs use container hostnames (`ollama:11434`, `mlflow:5000`) |

### ğŸš€ Execution Instructions

```bash
# 1. Build all images
docker compose build

# 2. Start the stack
docker compose up -d

# 3. Pull the LLM model into Ollama
docker exec -it ollama ollama pull qwen2.5:7b-instruct
```

### Service URLs

| Service | URL |
|---|---|
| REST API | `http://localhost:8002` |
| Health check | `http://localhost:8002/health` |
| gRPC | `localhost:50051` |
| MLflow UI | `http://localhost:5000` |
| Ollama | `http://localhost:11434` |

### Test the REST endpoint

```bash
curl -X POST http://localhost:8002/generate \
  -H "Content-Type: application/json" \
  -d '{
    "user_message": "Explain quantum computing in 3 sentences.",
    "system_prompt": "You are a helpful science tutor.",
    "context_json": "{}",
    "agent_settings": {
      "model_name": "",
      "temperature": 0.7,
      "max_tokens": 512
    }
  }'
```

### Running Servers Separately

```bash
# REST only (default Dockerfile CMD)
docker compose up ai-agent  # uses: uvicorn main:app

# gRPC only (override CMD)
# In docker-compose.yml change command to: ["python", "run_grpc.py"]
```

### Where to Plug in a Real RAG Retriever

1. Create `services/pgvector_retriever.py` implementing `BaseRetriever.retrieve()`
2. Add your vector DB connection settings to `core/config.py`
3. In `services/orchestrator.py`, swap `DummyRetriever()` for your implementation
4. Set `RAG_ENABLED=true` in `.env`