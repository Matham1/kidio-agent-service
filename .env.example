# ─────────────────────────────────────────────────────────────────────
# AI Agent Microservice — Environment Variables
# ─────────────────────────────────────────────────────────────────────
# Copy this file to .env and adjust values as needed.
#
#   cp .env.example .env
#
# IMPORTANT: All URLs use container hostnames, NOT localhost.
# ─────────────────────────────────────────────────────────────────────

# ── Service ──────────────────────────────────────────────────────────
SERVICE_NAME=ai-agent
ENVIRONMENT=development
DEBUG=false

# ── Ollama (LLM Runtime) ────────────────────────────────────────────
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=qwen2.5:7b-instruct

# ── LLM Defaults ────────────────────────────────────────────────────
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=2048
LLM_TIMEOUT_SECONDS=120
LLM_MAX_RETRIES=3

# ── MLflow (Observability) ──────────────────────────────────────────
MLFLOW_TRACKING_URI=http://mlflow:5000
MLFLOW_EXPERIMENT_NAME=ai-agent-generations

# ── Transport ────────────────────────────────────────────────────────
REST_HOST=0.0.0.0
REST_PORT=8002
GRPC_HOST=0.0.0.0
GRPC_PORT=50051
GRPC_MAX_WORKERS=10

# ── RAG (future) ────────────────────────────────────────────────────
RAG_ENABLED=false
RAG_TOP_K=5
